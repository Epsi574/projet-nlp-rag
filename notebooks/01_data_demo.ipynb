{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8743948a",
   "metadata": {},
   "source": [
    "# Pipeline de Traitement des Données - Démonstration\n",
    "\n",
    "Ce notebook démontre l'utilisation de la pipeline de traitement des données pour le projet RAG.\n",
    "\n",
    "**Objectifs:**\n",
    "- Scanner et cataloguer les documents\n",
    "- Nettoyer les textes\n",
    "- Découper en chunks optimisés\n",
    "- Générer les embeddings\n",
    "- Analyser les statistiques et la qualité\n",
    "\n",
    "**Organisation:**\n",
    "- Le code de traitement est maintenant dans `src/data_processor.py`\n",
    "- Ce notebook sert à la visualisation et l'analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78412c98",
   "metadata": {},
   "source": [
    "## 0. Setup et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32a0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace: H:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path(\"..\").resolve()\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from src.data_processor import DataProcessor\n",
    "\n",
    "print(f\"Workspace: {ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dee933",
   "metadata": {},
   "source": [
    "## 1. Initialisation de la pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660e9f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processeur initialisé\n",
      "  - Répertoire data: H:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\\data\n",
      "  - Répertoire raw: H:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\\data\\raw\n",
      "  - Répertoire clean: H:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\\data\\clean\n"
     ]
    }
   ],
   "source": [
    "# Créer le processeur de données\n",
    "data_dir = ROOT / \"data\"\n",
    "processor = DataProcessor(data_dir)\n",
    "\n",
    "print(\"Processeur initialisé\")\n",
    "print(f\"  - Répertoire data: {processor.data_dir}\")\n",
    "print(f\"  - Répertoire raw: {processor.raw_dir}\")\n",
    "print(f\"  - Répertoire clean: {processor.clean_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed800f",
   "metadata": {},
   "source": [
    "## 2. Scan et catalogage des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737c6c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc001: Armistice à Bordeaux | Source: Wikisource | Langue: fr\n",
      "doc002: Convention d’armistice franco-allemande | Source: Wikisource | Langue: fr\n",
      "doc003: Déclaration interalliée du 17 décembre 1942 | Source: Wikisource | Langue: fr\n",
      "doc004: Le racisme hitlérien, machine de guerre contre la France | Source: Wikisource | Langue: fr\n",
      "doc005: Les services statistiques français pendant l’Occupation | Source: Wikisource | Langue: fr\n",
      "doc006: Témoignage (Lebrun) | Source: Wikisource | Langue: fr\n",
      "\n",
      "Total: 6 documents\n",
      "Sauvegardé: H:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\\data\\docs.csv\n",
      "\n",
      "======================================================================\n",
      "Catalogue des documents\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>local_path</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc001</td>\n",
       "      <td>Armistice à Bordeaux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc002</td>\n",
       "      <td>Convention d’armistice franco-allemande</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc003</td>\n",
       "      <td>Déclaration interalliée du 17 décembre 1942</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc004</td>\n",
       "      <td>Le racisme hitlérien, machine de guerre contre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc005</td>\n",
       "      <td>Les services statistiques français pendant l’O...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc006</td>\n",
       "      <td>Témoignage (Lebrun)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title date author  \\\n",
       "0  doc001                               Armistice à Bordeaux               \n",
       "1  doc002            Convention d’armistice franco-allemande               \n",
       "2  doc003        Déclaration interalliée du 17 décembre 1942               \n",
       "3  doc004  Le racisme hitlérien, machine de guerre contre...               \n",
       "4  doc005  Les services statistiques français pendant l’O...               \n",
       "5  doc006                                Témoignage (Lebrun)               \n",
       "\n",
       "       source url                                         local_path language  \n",
       "0  Wikisource      H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...       fr  \n",
       "1  Wikisource      H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...       fr  \n",
       "2  Wikisource      H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...       fr  \n",
       "3  Wikisource      H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...       fr  \n",
       "4  Wikisource      H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...       fr  \n",
       "5  Wikisource      H:/Documents/cyTech/Ing3/NLP/projet-nlp-rag/da...       fr  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scanner les documents dans raw/\n",
    "df_docs = processor.scan_documents(verbose=True)\n",
    "\n",
    "# Afficher le catalogue\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Catalogue des documents\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "display(df_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a780fcf",
   "metadata": {},
   "source": [
    "## 3. Nettoyage des documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4adc7168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Armistice_à_Bordeaux.txt -> Armistice_a_Bordeaux.txt\n",
      "Convention_d’armistice_franco-allemande.txt -> Convention_darmistice_franco-allemande.txt\n",
      "Déclaration_interalliée_du_17_décembre_1942.txt -> Declaration_interalliee_du_17_decembre_1942.txt\n",
      "Le_racisme_hitlérien,_machine_de_guerre_contre_la_France.txt -> Le_racisme_hitlerien_machine_de_guerre_contre_la_France.txt\n",
      "Les_services_statistiques_français_pendant_l’Occupation.txt -> Les_services_statistiques_francais_pendant_lOccupation.txt\n",
      "Témoignage_(Lebrun).txt -> Temoignage_Lebrun.txt\n",
      "\n",
      "Cleaned files in: H:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\\data\\clean\n",
      "\n",
      "6 documents nettoyés\n"
     ]
    }
   ],
   "source": [
    "# Nettoyer tous les documents\n",
    "cleaned_count = processor.clean_documents(verbose=True)\n",
    "\n",
    "print(f\"\\n{cleaned_count} documents nettoyés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fba92",
   "metadata": {},
   "source": [
    "## 4. Génération des chunks\n",
    "\n",
    "**Paramètres de chunking:**\n",
    "- `chunk_size`: 2000 caractères (~300-500 tokens)\n",
    "- `overlap`: 200 caractères (~30-50 tokens)\n",
    "\n",
    "**Stratégie:** Découpe intelligente par paragraphe > phrase > ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a39b7e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 1472\n",
      "Wrote: H:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\\data\\chunks.jsonl\n",
      "\n",
      "Total de chunks générés: 1472\n"
     ]
    }
   ],
   "source": [
    "# Générer les chunks\n",
    "df_chunks = processor.generate_chunks(\n",
    "    chunk_size=2000,\n",
    "    overlap=200,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal de chunks générés: {len(df_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa892ad",
   "metadata": {},
   "source": [
    "## 5. Analyse statistique des chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a188691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STATISTIQUES GLOBALES\n",
      "======================================================================\n",
      "\n",
      "Documents sources    : 6\n",
      "Chunks générés       : 1472\n",
      "Moyenne chunks/doc   : 245.3\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "DISTRIBUTION DES LONGUEURS\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Métrique</th>\n",
       "      <th>Moyenne</th>\n",
       "      <th>Médiane</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Écart-type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Caractères</td>\n",
       "      <td>677</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>1999</td>\n",
       "      <td>787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mots</td>\n",
       "      <td>111</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tokens (estimé)</td>\n",
       "      <td>169</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>499</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Métrique Moyenne Médiane Min   Max Écart-type\n",
       "0       Caractères     677     161   1  1999        787\n",
       "1             Mots     111      28   1   365        128\n",
       "2  Tokens (estimé)     169      40   0   499        197"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ajouter une colonne pour la longueur\n",
    "df_chunks[\"len_chars\"] = df_chunks[\"text\"].str.len()\n",
    "df_chunks[\"len_words\"] = df_chunks[\"text\"].str.split().str.len()\n",
    "df_chunks[\"len_tokens_est\"] = (df_chunks[\"len_chars\"] / 4).astype(int)\n",
    "\n",
    "# Statistiques globales\n",
    "print(\"=\" * 70)\n",
    "print(\"STATISTIQUES GLOBALES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nb_docs = df_chunks[\"doc_id\"].nunique()\n",
    "nb_chunks = len(df_chunks)\n",
    "\n",
    "print(f\"\\nDocuments sources    : {nb_docs}\")\n",
    "print(f\"Chunks générés       : {nb_chunks}\")\n",
    "print(f\"Moyenne chunks/doc   : {nb_chunks / nb_docs:.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"DISTRIBUTION DES LONGUEURS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "stats_df = pd.DataFrame({\n",
    "    \"Métrique\": [\"Caractères\", \"Mots\", \"Tokens (estimé)\"],\n",
    "    \"Moyenne\": [\n",
    "        f\"{df_chunks['len_chars'].mean():.0f}\",\n",
    "        f\"{df_chunks['len_words'].mean():.0f}\",\n",
    "        f\"{df_chunks['len_tokens_est'].mean():.0f}\"\n",
    "    ],\n",
    "    \"Médiane\": [\n",
    "        f\"{df_chunks['len_chars'].median():.0f}\",\n",
    "        f\"{df_chunks['len_words'].median():.0f}\",\n",
    "        f\"{df_chunks['len_tokens_est'].median():.0f}\"\n",
    "    ],\n",
    "    \"Min\": [\n",
    "        f\"{df_chunks['len_chars'].min():.0f}\",\n",
    "        f\"{df_chunks['len_words'].min():.0f}\",\n",
    "        f\"{df_chunks['len_tokens_est'].min():.0f}\"\n",
    "    ],\n",
    "    \"Max\": [\n",
    "        f\"{df_chunks['len_chars'].max():.0f}\",\n",
    "        f\"{df_chunks['len_words'].max():.0f}\",\n",
    "        f\"{df_chunks['len_tokens_est'].max():.0f}\"\n",
    "    ],\n",
    "    \"Écart-type\": [\n",
    "        f\"{df_chunks['len_chars'].std():.0f}\",\n",
    "        f\"{df_chunks['len_words'].std():.0f}\",\n",
    "        f\"{df_chunks['len_tokens_est'].std():.0f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47062067",
   "metadata": {},
   "source": [
    "## 6. Répartition par document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88b1912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "RÉPARTITION DÉTAILLÉE PAR DOCUMENT\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>titre</th>\n",
       "      <th>nb_chunks</th>\n",
       "      <th>chars_moy</th>\n",
       "      <th>chars_total</th>\n",
       "      <th>tokens_moy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc001</td>\n",
       "      <td>Armistice à Bordeaux</td>\n",
       "      <td>209</td>\n",
       "      <td>221</td>\n",
       "      <td>46128</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc002</td>\n",
       "      <td>Convention d’armistice franco-allemande</td>\n",
       "      <td>156</td>\n",
       "      <td>218</td>\n",
       "      <td>34083</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc003</td>\n",
       "      <td>Déclaration interalliée du 17 décembre 1...</td>\n",
       "      <td>69</td>\n",
       "      <td>196</td>\n",
       "      <td>13515</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc004</td>\n",
       "      <td>Le racisme hitlérien, machine de guerre ...</td>\n",
       "      <td>212</td>\n",
       "      <td>209</td>\n",
       "      <td>44299</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc005</td>\n",
       "      <td>Les services statistiques français penda...</td>\n",
       "      <td>284</td>\n",
       "      <td>834</td>\n",
       "      <td>236825</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc006</td>\n",
       "      <td>Témoignage (Lebrun)</td>\n",
       "      <td>542</td>\n",
       "      <td>1147</td>\n",
       "      <td>621487</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                        titre  nb_chunks chars_moy  \\\n",
       "0  doc001                         Armistice à Bordeaux        209       221   \n",
       "1  doc002      Convention d’armistice franco-allemande        156       218   \n",
       "2  doc003  Déclaration interalliée du 17 décembre 1...         69       196   \n",
       "3  doc004  Le racisme hitlérien, machine de guerre ...        212       209   \n",
       "4  doc005  Les services statistiques français penda...        284       834   \n",
       "5  doc006                          Témoignage (Lebrun)        542      1147   \n",
       "\n",
       "  chars_total tokens_moy  \n",
       "0       46128         55  \n",
       "1       34083         54  \n",
       "2       13515         49  \n",
       "3       44299         52  \n",
       "4      236825        208  \n",
       "5      621487        286  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tableau détaillé par document\n",
    "print(\"-\" * 70)\n",
    "print(\"RÉPARTITION DÉTAILLÉE PAR DOCUMENT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "doc_stats = []\n",
    "for doc_id in df_chunks[\"doc_id\"].unique():\n",
    "    doc_data = df_chunks[df_chunks[\"doc_id\"] == doc_id]\n",
    "    title = doc_data[\"title\"].iloc[0] if pd.notna(doc_data[\"title\"].iloc[0]) else doc_id\n",
    "    \n",
    "    doc_stats.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"titre\": title[:40] + \"...\" if len(str(title)) > 40 else title,\n",
    "        \"nb_chunks\": len(doc_data),\n",
    "        \"chars_moy\": f\"{doc_data['len_chars'].mean():.0f}\",\n",
    "        \"chars_total\": f\"{doc_data['len_chars'].sum():.0f}\",\n",
    "        \"tokens_moy\": f\"{doc_data['len_tokens_est'].mean():.0f}\",\n",
    "    })\n",
    "\n",
    "df_doc_stats = pd.DataFrame(doc_stats)\n",
    "display(df_doc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0769345",
   "metadata": {},
   "source": [
    "## 7. Exemples de chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "425c1f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXEMPLES DE CHUNKS (vérification manuelle)\n",
      "======================================================================\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "CHUNK #1\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "ID          : doc001_0094\n",
      "Document    : Armistice à Bordeaux\n",
      "Date        : N/A\n",
      "Source      : Wikisource\n",
      "Longueur    : 119 chars / 20 mots / ~29 tokens\n",
      "Position    : [23556 - 23677]\n",
      "\n",
      "Extrait (200 premiers caractères):\n",
      "d, et dont les oreilles perçoivent en orgue et en tonnerre chaque syllabe de l’hymne que récitent mes lèvres immobiles....\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "CHUNK #2\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "ID          : doc006_0166\n",
      "Document    : Témoignage (Lebrun)\n",
      "Date        : N/A\n",
      "Source      : Wikisource\n",
      "Longueur    : 1632 chars / 257 mots / ~408 tokens\n",
      "Position    : [258362 - 259996]\n",
      "\n",
      "Extrait (200 premiers caractères):\n",
      "ssaire, oui, mais dans le cadre général de la République démocratique.\n",
      "\n",
      "Ce qui a compliqué la situation issue du vote de l’Assemblée nationale, c’est la durée du régime provisoire instauré par elle.\n",
      "\n",
      "...\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "CHUNK #3\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "ID          : doc002_0017\n",
      "Document    : Convention d’armistice franco-allemande\n",
      "Date        : N/A\n",
      "Source      : Wikisource\n",
      "Longueur    : 191 chars / 29 mots / ~47 tokens\n",
      "Position    : [13684 - 13877]\n",
      "\n",
      "Extrait (200 premiers caractères):\n",
      "tre dénoncée à tout moment pour prendre fin immédiatement par le gouvernement allemand si le gouvernement français ne remplit pas les obligations par lui assumées dans la présente convention....\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Afficher quelques exemples aléatoires\n",
    "print(\"=\" * 70)\n",
    "print(\"EXEMPLES DE CHUNKS (vérification manuelle)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sample_chunks = df_chunks.sample(min(3, len(df_chunks)))\n",
    "\n",
    "for idx, (_, chunk) in enumerate(sample_chunks.iterrows(), 1):\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"CHUNK #{idx}\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "    print(f\"ID          : {chunk['chunk_id']}\")\n",
    "    print(f\"Document    : {chunk['title'] if pd.notna(chunk['title']) and chunk['title'] else chunk['doc_id']}\")\n",
    "    print(f\"Date        : {chunk['date'] if pd.notna(chunk['date']) and chunk['date'] else 'N/A'}\")\n",
    "    print(f\"Source      : {chunk['source']}\")\n",
    "    print(f\"Longueur    : {chunk['len_chars']} chars / {chunk['len_words']} mots / ~{chunk['len_tokens_est']} tokens\")\n",
    "    print(f\"Position    : [{chunk['start_char']} - {chunk['end_char']}]\")\n",
    "    print(f\"\\nExtrait (200 premiers caractères):\")\n",
    "    print(f\"{chunk['text'][:200]}...\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c9603",
   "metadata": {},
   "source": [
    "## 8. Génération des embeddings\n",
    "\n",
    "**Modèle:** `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Dimension: 384\n",
    "- Rapide et performant pour la plupart des tâches\n",
    "- Normalisé pour similarité cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e0b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération des embeddings pour 1472 chunks...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Générer les embeddings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m embeddings, chunk_ids \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInformations sur les embeddings:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Nombre de vecteurs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mH:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\\src\\data_processor.py:496\u001b[0m, in \u001b[0;36mDataProcessor.generate_embeddings\u001b[1;34m(self, model_name, batch_size, verbose)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGénération des embeddings pour \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# Initialiser l'embedder\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m embedder \u001b[38;5;241m=\u001b[39m \u001b[43mEmbedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Générer les embeddings\u001b[39;00m\n\u001b[0;32m    499\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedder\u001b[38;5;241m.\u001b[39mencode(texts, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32mH:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\\src\\embeddings.py:13\u001b[0m, in \u001b[0;36mEmbedder.__init__\u001b[1;34m(self, model_name, normalize)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, normalize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m SentenceTransformer(model_name)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;241m=\u001b[39m normalize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# Générer les embeddings\n",
    "embeddings, chunk_ids = processor.generate_embeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    batch_size=64,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nInformations sur les embeddings:\")\n",
    "print(f\"  - Nombre de vecteurs: {embeddings.shape[0]}\")\n",
    "print(f\"  - Dimension: {embeddings.shape[1]}\")\n",
    "print(f\"  - Type: {embeddings.dtype}\")\n",
    "print(f\"  - Taille mémoire: {embeddings.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6ee17",
   "metadata": {},
   "source": [
    "## 9. Vérifications de qualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66caf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifications finales\n",
    "print(\"=\" * 70)\n",
    "print(\"CHECKLIST DE QUALITÉ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "checks = []\n",
    "\n",
    "# 1. Fichiers générés\n",
    "checks.append((\"docs.csv existe\", processor.docs_csv.exists()))\n",
    "checks.append((\"chunks.jsonl existe\", processor.chunks_jsonl.exists()))\n",
    "checks.append((\"embeddings.npy existe\", processor.embeddings_npy.exists()))\n",
    "checks.append((\"chunk_ids.npy existe\", processor.chunk_ids_npy.exists()))\n",
    "\n",
    "# 2. Cohérence des données\n",
    "checks.append((\"Nombre chunks = nombre embeddings\", len(df_chunks) == len(embeddings)))\n",
    "checks.append((\"Nombre chunk_ids = nombre embeddings\", len(chunk_ids) == len(embeddings)))\n",
    "\n",
    "# 3. Qualité des chunks\n",
    "empty_chunks = (df_chunks[\"text\"].str.strip().str.len() == 0).sum()\n",
    "checks.append((\"Pas de chunks vides\", empty_chunks == 0))\n",
    "\n",
    "missing_sources = (df_chunks[\"source\"].fillna(\"\") == \"\").sum()\n",
    "checks.append((f\"Sources renseignées ({len(df_chunks) - missing_sources}/{len(df_chunks)})\", missing_sources == 0))\n",
    "\n",
    "# 4. Embeddings\n",
    "embeddings_normalized = np.allclose(np.linalg.norm(embeddings, axis=1), 1.0, atol=1e-5)\n",
    "checks.append((\"Embeddings normalisés (L2)\", embeddings_normalized))\n",
    "\n",
    "# Afficher les résultats\n",
    "for check_name, passed in checks:\n",
    "    status = \"[OK]\" if passed else \"[FAIL]\"\n",
    "    print(f\"{status} {check_name}\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "\n",
    "all_passed = all(passed for _, passed in checks)\n",
    "if all_passed:\n",
    "    print(\"TOUS LES CONTRÔLES SONT PASSÉS !\")\n",
    "    print(\"Dataset prêt pour l'indexation et la recherche.\")\n",
    "else:\n",
    "    print(\"ATTENTION: Certains contrôles ont échoué. Vérifier les détails ci-dessus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccebbb8b",
   "metadata": {},
   "source": [
    "## Résumé de la pipeline\n",
    "\n",
    "**Fichiers générés:**\n",
    "1. `data/docs.csv` - Catalogue des documents sources\n",
    "2. `data/chunks.jsonl` - Tous les chunks avec métadonnées\n",
    "3. `data/embeddings.npy` - Vecteurs d'embeddings (normalisés)\n",
    "4. `data/chunk_ids.npy` - IDs correspondants aux embeddings\n",
    "\n",
    "**Prochaines étapes:**\n",
    "1. Indexation avec FAISS (voir `src/index.py`)\n",
    "2. Implémentation du retriever (voir `src/retriever.py`)\n",
    "3. Intégration avec le LLM pour la génération\n",
    "4. Évaluation et benchmarking\n",
    "\n",
    "**Pour réexécuter la pipeline complète en ligne de commande:**\n",
    "```bash\n",
    "python -m src.scripts.build_data_pipeline --chunk-size 2000 --overlap 200\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
