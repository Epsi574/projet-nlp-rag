{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f16fdd",
   "metadata": {},
   "source": [
    "# Notebook de démonstration et d'évaluation RAG\n",
    "\n",
    "Ce notebook compare les réponses générées par un modèle baseline et par un pipeline RAG (Retrieval-Augmented Generation) sur un jeu de questions. Les sources utilisées par le RAG sont également affichées pour chaque réponse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad194c",
   "metadata": {},
   "source": [
    "# 0. Importer les bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1109af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import du pipeline de traitement\n",
    "from src.data_processor import DataProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d78cc",
   "metadata": {},
   "source": [
    "# 1. Lancer la pipeline de préparation des données\n",
    "\n",
    "On exécute la pipeline complète (scan, clean, chunk, embeddings) pour préparer les données à partir des fichiers bruts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancer la pipeline de préparation des données via le script CLI\n",
    "!python -m src.scripts.build_data_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fec477",
   "metadata": {},
   "source": [
    "## 2. Définir les questions de test\n",
    "\n",
    "Nous définissons ici une liste de 10 questions représentatives pour l'évaluation du système."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection de 10 questions variées pour l'évaluation\n",
    "questions = [\n",
    "    \"Quand débute l’entre-deux-guerres ?\",\n",
    "    \"Quel traité met officiellement fin à la Première Guerre mondiale avec l’Allemagne ?\",\n",
    "    \"Dans quelle ville est signé le traité de Versailles ?\",\n",
    "    \"Quelle faiblesse structurelle empêche la SDN d’agir efficacement ?\",\n",
    "    \"Quelle date marque le début de la Seconde Guerre mondiale ?\",\n",
    "    \"Quel objectif idéologique central justifie l’expansion allemande ?\",\n",
    "    \"Quel parti politique dirige l’Allemagne à partir de 1933 ?\",\n",
    "    \"Quel territoire est remilitarisé par l’Allemagne en 1936 ?\",\n",
    "    \"Qui devient chancelier de l’Allemagne en janvier 1933 ?\",\n",
    "    \"Quel accord permet à l’Allemagne d’annexer les Sudètes ?\"\n",
    "]\n",
    "\n",
    "# Affichage des questions\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"Q{i}: {q}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f6f3a",
   "metadata": {},
   "source": [
    "## 3. Définir la fonction de génération de réponse baseline\n",
    "\n",
    "La fonction ci-dessous simule une génération de réponse baseline (par exemple, un LLM sans accès aux documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb79ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction baseline utilisant les mêmes réglages LLM que run_rag.py ou run_baseline.py\n",
    "from src.llm_client import LLMClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_url = os.getenv(\"LLM_API_URL\", \"\").strip()\n",
    "llm_model = os.getenv(\"LLM_MODEL\", \"llama3\").strip()\n",
    "\n",
    "client = LLMClient(api_url, llm_model)\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"\"\"Tu es un assistant. Réponds de façon concise et factuelle.\\nSi tu n'es pas sûr, dis-le explicitement.\\n\"\"\"\n",
    "\n",
    "def build_baseline_prompt(question: str) -> str:\n",
    "    return f\"{BASELINE_SYSTEM_PROMPT}\\nQuestion: {question}\\nRéponse:\"\n",
    "\n",
    "def generate_baseline_answer(question: str) -> str:\n",
    "    prompt = build_baseline_prompt(question)\n",
    "    return client.generate(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c23a3",
   "metadata": {},
   "source": [
    "## 4. Définir la fonction de génération de réponse RAG avec sources\n",
    "\n",
    "La fonction suivante simule une génération de réponse RAG et retourne aussi les sources utilisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction RAG : version hybride dense + BM25 (alignée sur run_rag.py)\n",
    "import json\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "\n",
    "# Tokenisation BM25 (identique script)\n",
    "_WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ0-9_]+\", re.UNICODE)\n",
    "def tokenize(text):\n",
    "    return _WORD_RE.findall(text.lower())\n",
    "\n",
    "# Chargement des chunks\n",
    "chunks = []\n",
    "texts = []\n",
    "with open('data/chunks.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        chunks.append({\n",
    "            \"id\": item.get(\"chunk_id\", \"?\"),\n",
    "            \"text\": item[\"text\"],\n",
    "            \"source\": item.get(\"title\", item.get(\"source\", \"Unknown\"))\n",
    "        })\n",
    "        texts.append(item[\"text\"])\n",
    "\n",
    "# Embeddings + index dense\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "dim = embeddings.shape[1]\n",
    "index = hnswlib.Index(space='cosine', dim=dim)\n",
    "index.init_index(max_elements=len(texts), ef_construction=200, M=16)\n",
    "index.add_items(embeddings, np.arange(len(texts)))\n",
    "index.set_ef(50)\n",
    "\n",
    "# Index BM25\n",
    "tokenized_corpus = [tokenize(t) for t in texts]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Fusion RRF\n",
    "def rrf_fuse(dense_ranked_ids, bm25_ranked_ids, rrf_k=60):\n",
    "    scores = {}\n",
    "    for rank, idx in enumerate(dense_ranked_ids, start=1):\n",
    "        scores[idx] = scores.get(idx, 0.0) + 1.0 / (rrf_k + rank)\n",
    "    for rank, idx in enumerate(bm25_ranked_ids, start=1):\n",
    "        scores[idx] = scores.get(idx, 0.0) + 1.0 / (rrf_k + rank)\n",
    "    return [idx for idx, _ in sorted(scores.items(), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "def dense_search(query, k):\n",
    "    q_vec = model.encode([query], convert_to_numpy=True)\n",
    "    q_vec = q_vec / np.linalg.norm(q_vec, axis=1, keepdims=True)\n",
    "    labels, _ = index.knn_query(q_vec, k=k)\n",
    "    return [int(i) for i in labels[0]]\n",
    "\n",
    "def bm25_search(query, k):\n",
    "    q_tok = tokenize(query)\n",
    "    scores = bm25.get_scores(q_tok)\n",
    "    top_idx = np.argsort(scores)[::-1][:k]\n",
    "    return [int(i) for i in top_idx]\n",
    "\n",
    "def hybrid_search(query, k_dense=10, k_bm25=10, k_final=5, rrf_k=60):\n",
    "    dense_ids = dense_search(query, k=k_dense)\n",
    "    bm25_ids = bm25_search(query, k=k_bm25)\n",
    "    fused_ids = rrf_fuse(dense_ids, bm25_ids, rrf_k=rrf_k)[:k_final]\n",
    "    results = []\n",
    "    for i in fused_ids:\n",
    "        results.append({\n",
    "            \"id\": chunks[i][\"id\"],\n",
    "            \"text\": chunks[i][\"text\"],\n",
    "            \"source\": chunks[i][\"source\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "BASELINE_SYSTEM_PROMPT = \"\"\"Tu es un assistant de questions-réponses.\\n\\nRègles strictes :\\n1) Réponds UNIQUEMENT à partir du CONTEXTE fourni.\\n2) Si l'information n'est pas présente dans le contexte, réponds : \\\"Je ne peux pas répondre avec certitude à partir des sources fournies.\\\"\\n3) Ne complète pas avec des connaissances externes.\\n4) Donne une réponse concise, puis liste les sources sous forme de puces.\\n\\nFormat de sortie :\\nRéponse : <ta réponse>\\n\\nSources :\\n- <doc_id ou titre> (chunk=<id>)\\n- ...\\n\"\"\"\n",
    "\n",
    "def build_rag_prompt(question, res):\n",
    "    context = \"\"\n",
    "    for r in res:\n",
    "        context += f\"SOURCE: {r['source']} (chunk={r.get('id','?')})\\nTEXT: {r['text']}\\n{'-'*60}\\n\"\n",
    "    prompt = f\"{BASELINE_SYSTEM_PROMPT}\\nCONTEXTE :\\n{context}\\nQUESTION :\\n{question}\"\n",
    "    return prompt\n",
    "\n",
    "def generate_rag_answer(question):\n",
    "    retrieved_chunks = hybrid_search(question, k_dense=10, k_bm25=10, k_final=3, rrf_k=60)\n",
    "    prompt = build_rag_prompt(question, retrieved_chunks)\n",
    "    answer = client.generate(prompt)\n",
    "    sources = [f\"{chunk['source']} (chunk={chunk['id']})\" for chunk in retrieved_chunks]\n",
    "    return answer, sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d684d",
   "metadata": {},
   "source": [
    "## 5. Générer et afficher les réponses baseline\n",
    "\n",
    "On boucle sur les questions de test et on affiche les réponses générées par la méthode baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération des réponses baseline\n",
    "baseline_answers = []\n",
    "for q in questions:\n",
    "    ans = generate_baseline_answer(q)\n",
    "    baseline_answers.append(ans)\n",
    "    print(f\"Q: {q}\\nBaseline: {ans}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f78b53",
   "metadata": {},
   "source": [
    "## 6. Générer et afficher les réponses RAG avec sources\n",
    "\n",
    "On boucle sur les questions de test et on affiche les réponses générées par la méthode RAG ainsi que les sources associées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92295526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération des réponses RAG\n",
    "rag_answers = []\n",
    "rag_sources = []\n",
    "for q in questions:\n",
    "    ans, sources = generate_rag_answer(q)\n",
    "    rag_answers.append(ans)\n",
    "    rag_sources.append(sources)\n",
    "    print(f\"Q: {q}\\nRAG: {ans}\\nSources: {sources}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01403d6b",
   "metadata": {},
   "source": [
    "## 7. Comparer les réponses baseline et RAG\n",
    "\n",
    "Affichage côte à côte des réponses baseline et RAG pour chaque question afin de faciliter la comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe74636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage comparatif des réponses\n",
    "for i, q in enumerate(questions):\n",
    "    print(f\"Q{i+1}: {q}\")\n",
    "    print(f\"  Baseline: {baseline_answers[i]}\")\n",
    "    print(f\"  RAG     : {rag_answers[i]}\")\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03f691",
   "metadata": {},
   "source": [
    "## 8. Afficher les sources utilisées pour chaque réponse RAG\n",
    "\n",
    "Pour chaque question, on affiche la liste des sources utilisées par la méthode RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ccb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des sources utilisées pour chaque réponse RAG\n",
    "for i, q in enumerate(questions):\n",
    "    print(f\"Q{i+1}: {q}\")\n",
    "    print(f\"  Sources RAG: {rag_sources[i]}\")\n",
    "    print('-'*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
