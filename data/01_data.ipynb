{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8ead00",
   "metadata": {},
   "source": [
    "# Cellule 0 — Setup & dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0fe95bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: H:\\Documents\\cyTech\\Ing3\\NLP\\projet-nlp-rag\\data\\data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re, json, hashlib\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "DATA = ROOT / \"data\"\n",
    "RAW  = DATA / \"raw\"\n",
    "CLEAN = DATA / \"clean\"\n",
    "\n",
    "RAW.mkdir(parents=True, exist_ok=True)\n",
    "CLEAN.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DOCS_CSV = DATA / \"docs.csv\"\n",
    "CHUNKS_JSONL = DATA / \"chunks.jsonl\"\n",
    "\n",
    "print(\"OK:\", DATA.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdfc59c",
   "metadata": {},
   "source": [
    "# Cellule 1 — Convention de métadonnées (schéma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dbe4f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_SCHEMA = [\n",
    "    \"doc_id\", \"title\", \"date\", \"author\",\n",
    "    \"source\", \"url\", \"local_path\", \"language\"\n",
    "]\n",
    "\n",
    "CHUNK_SCHEMA = [\n",
    "    \"chunk_id\", \"doc_id\", \"text\",\n",
    "    \"title\", \"date\", \"author\", \"source\", \"url\",\n",
    "    \"start_char\", \"end_char\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd003c",
   "metadata": {},
   "source": [
    "# Cellule 2 — Ajouter des docs (manuel au début, puis automatisable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e129dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc001: Armistice à Bordeaux | Source: Wikisource | Langue: fr\n",
      "doc002: Convention d’armistice franco-allemande | Source: Wikisource | Langue: fr\n",
      "doc003: Déclaration interalliée du 17 décembre 1942 | Source: Wikisource | Langue: fr\n",
      "doc004: Le racisme hitlérien, machine de guerre contre la France | Source: Wikisource | Langue: fr\n",
      "doc005: Les services statistiques français pendant l’Occupation | Source: Wikisource | Langue: fr\n",
      "doc006: Témoignage (Lebrun) | Source: Wikisource | Langue: fr\n",
      "\n",
      "Total: 6 documents\n",
      "A completer manuellement: date, author, url\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>local_path</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc001</td>\n",
       "      <td>Armistice à Bordeaux</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>data/raw/Armistice_à_Bordeaux.txt</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc002</td>\n",
       "      <td>Convention d’armistice franco-allemande</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>data/raw/Convention_d’armistice_franco-alleman...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc003</td>\n",
       "      <td>Déclaration interalliée du 17 décembre 1942</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>data/raw/Déclaration_interalliée_du_17_décembr...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc004</td>\n",
       "      <td>Le racisme hitlérien, machine de guerre contre...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>data/raw/Le_racisme_hitlérien,_machine_de_guer...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc005</td>\n",
       "      <td>Les services statistiques français pendant l’O...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>data/raw/Les_services_statistiques_français_pe...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>doc006</td>\n",
       "      <td>Témoignage (Lebrun)</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Wikisource</td>\n",
       "      <td></td>\n",
       "      <td>data/raw/Témoignage_(Lebrun).txt</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title date author  \\\n",
       "0  doc001                               Armistice à Bordeaux               \n",
       "1  doc002            Convention d’armistice franco-allemande               \n",
       "2  doc003        Déclaration interalliée du 17 décembre 1942               \n",
       "3  doc004  Le racisme hitlérien, machine de guerre contre...               \n",
       "4  doc005  Les services statistiques français pendant l’O...               \n",
       "5  doc006                                Témoignage (Lebrun)               \n",
       "\n",
       "       source url                                         local_path language  \n",
       "0  Wikisource                      data/raw/Armistice_à_Bordeaux.txt       fr  \n",
       "1  Wikisource      data/raw/Convention_d’armistice_franco-alleman...       fr  \n",
       "2  Wikisource      data/raw/Déclaration_interalliée_du_17_décembr...       fr  \n",
       "3  Wikisource      data/raw/Le_racisme_hitlérien,_machine_de_guer...       fr  \n",
       "4  Wikisource      data/raw/Les_services_statistiques_français_pe...       fr  \n",
       "5  Wikisource                       data/raw/Témoignage_(Lebrun).txt       fr  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_auto_metadata(text: str, filename: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extrait automatiquement la source et la langue.\n",
    "    \"\"\"\n",
    "    header = text[:1000]\n",
    "    \n",
    "    # Déterminer la source\n",
    "    source = \"Archive historique\"\n",
    "    if \"wikisource\" in header.lower():\n",
    "        source = \"Wikisource\"\n",
    "    elif \"gallica\" in header.lower():\n",
    "        source = \"Gallica BnF\"\n",
    "    elif \"journal officiel\" in header.lower():\n",
    "        source = \"Journal Officiel\"\n",
    "    elif \"convention\" in filename.lower() or \"déclaration\" in filename.lower():\n",
    "        source = \"Document officiel\"\n",
    "    \n",
    "    # Déterminer la langue\n",
    "    language = \"fr\"\n",
    "    english_words = re.findall(r'\\b(the|and|of|in|to|for|with|that|this|from)\\b', header[:500], re.IGNORECASE)\n",
    "    french_words = re.findall(r'\\b(le|la|de|et|les|des|un|une|dans|pour|par|sur)\\b', header[:500], re.IGNORECASE)\n",
    "    \n",
    "    if len(english_words) > len(french_words) * 2:\n",
    "        language = \"en\"\n",
    "    \n",
    "    return {\n",
    "        \"source\": source,\n",
    "        \"language\": language\n",
    "    }\n",
    "\n",
    "\n",
    "# Scan automatique du dossier raw/\n",
    "docs = []\n",
    "doc_counter = 1\n",
    "\n",
    "for txt_file in sorted(RAW.glob(\"*.txt\")):\n",
    "    # Extraire le titre du nom de fichier (remplacer _ par espace)\n",
    "    title = txt_file.stem.replace(\"_\", \" \")\n",
    "    \n",
    "    # Générer un doc_id unique\n",
    "    doc_id = f\"doc{doc_counter:03d}\"\n",
    "    \n",
    "    # Lire le contenu pour extraire source et langue\n",
    "    try:\n",
    "        text_content = txt_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        metadata = extract_auto_metadata(text_content, txt_file.stem)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lecture {txt_file.name}: {e}\")\n",
    "        metadata = {\"source\": \"Archive historique\", \"language\": \"fr\"}\n",
    "    \n",
    "    # Créer l'entrée du document\n",
    "    doc_entry = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"title\": title,\n",
    "        \"date\": \"\",  # À remplir manuellement\n",
    "        \"author\": \"\",  # À remplir manuellement\n",
    "        \"source\": metadata[\"source\"],\n",
    "        \"url\": \"\",  # À remplir manuellement\n",
    "        \"local_path\": str(txt_file.as_posix()),\n",
    "        \"language\": metadata[\"language\"],\n",
    "    }\n",
    "    \n",
    "    docs.append(doc_entry)\n",
    "    doc_counter += 1\n",
    "    print(f\"{doc_id}: {title} | Source: {metadata['source']} | Langue: {metadata['language']}\")\n",
    "\n",
    "# Créer le DataFrame et sauvegarder\n",
    "df_docs = pd.DataFrame(docs, columns=DOC_SCHEMA)\n",
    "df_docs.to_csv(DOCS_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nTotal: {len(docs)} documents\")\n",
    "print(\"A completer manuellement: date, author, url\")\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260cc229",
   "metadata": {},
   "source": [
    "# Cellule 3 — Fonctions utilitaires (lecture + hashing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e74a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def safe_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalise un nom de fichier pour éviter les problèmes de caractères spéciaux.\n",
    "    Convertit les caractères accentués/spéciaux en ASCII safe.\n",
    "    \"\"\"\n",
    "    # Décompose les caractères Unicode (é -> e + ´)\n",
    "    nfd = unicodedata.normalize('NFD', filename)\n",
    "    # Garde uniquement les caractères ASCII\n",
    "    ascii_str = nfd.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remplace les caractères problématiques\n",
    "    replacements = {\n",
    "        ' ': '_',\n",
    "        '−': '-',\n",
    "        '–': '-',\n",
    "        '—': '-',\n",
    "        ''': \"'\",\n",
    "        ''': \"'\",\n",
    "        '\"': '\"',\n",
    "        '\"': '\"',\n",
    "        '(': '_',\n",
    "        ')': '_',\n",
    "        ',': '_',\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        ascii_str = ascii_str.replace(old, new)\n",
    "    \n",
    "    # Supprime les caractères multiples\n",
    "    ascii_str = re.sub(r'_+', '_', ascii_str)\n",
    "    ascii_str = re.sub(r'-+', '-', ascii_str)\n",
    "    \n",
    "    return ascii_str.strip('_-')\n",
    "\n",
    "def file_sha1(path: Path) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def read_text(path: Path) -> str:\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf1140",
   "metadata": {},
   "source": [
    "# Cellule 4 — Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6933d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # Supprimer les lignes HTML/meta\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        # Ignorer les balises HTML/meta\n",
    "        if stripped.startswith('<meta') or stripped.startswith('<link') or stripped.startswith('<!DOCTYPE'):\n",
    "            continue\n",
    "        filtered_lines.append(line)\n",
    "    \n",
    "    text = '\\n'.join(filtered_lines)\n",
    "    \n",
    "    # Couper tout après \"À propos de cette édition électronique\" et variantes\n",
    "    cutoff_phrases = [\n",
    "        \"À propos de cette édition électronique\",\n",
    "        \"à propos de cette édition électronique\",\n",
    "        \"À PROPOS DE CETTE ÉDITION\",\n",
    "        \"Voir aussi\",\n",
    "        \"Notes et références\",\n",
    "        \"Source\",\n",
    "        \"Cette édition électronique\",\n",
    "    ]\n",
    "    \n",
    "    for phrase in cutoff_phrases:\n",
    "        idx = text.find(phrase)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "            break  # Couper à la première occurrence trouvée\n",
    "    \n",
    "    # Normalisation basique\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)           # espaces multiples\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)        # trop de sauts de ligne\n",
    "    text = text.strip()\n",
    "\n",
    "    # Retirer césures de fin de ligne \"exem-\\nple\" -> \"exemple\"\n",
    "    text = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81343bbe",
   "metadata": {},
   "source": [
    "# Cellule 5 — Appliquer nettoyage à tous les docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12c0b133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Armistice_à_Bordeaux.txt -> Armistice_a_Bordeaux.txt\n",
      "Convention_d’armistice_franco-allemande.txt -> Convention_darmistice_franco-allemande.txt\n",
      "Déclaration_interalliée_du_17_décembre_1942.txt -> Declaration_interalliee_du_17_decembre_1942.txt\n",
      "Le_racisme_hitlérien,_machine_de_guerre_contre_la_France.txt -> Le_racisme_hitlerien_machine_de_guerre_contre_la_France.txt\n",
      "Les_services_statistiques_français_pendant_l’Occupation.txt -> Les_services_statistiques_francais_pendant_lOccupation.txt\n",
      "Témoignage_(Lebrun).txt -> Temoignage_Lebrun.txt\n",
      "\n",
      "Cleaned files in: data\\clean\n"
     ]
    }
   ],
   "source": [
    "df_docs = pd.read_csv(DOCS_CSV)\n",
    "\n",
    "for _, row in df_docs.iterrows():\n",
    "    raw_path = Path(row[\"local_path\"])\n",
    "    if not raw_path.exists():\n",
    "        print(\"MISSING:\", raw_path)\n",
    "        continue\n",
    "\n",
    "    txt = read_text(raw_path)\n",
    "    txt_clean = clean_text(txt)\n",
    "\n",
    "    # Normaliser le nom de fichier pour éviter les SKIP\n",
    "    safe_name = safe_filename(raw_path.stem) + \".txt\"\n",
    "    clean_path = CLEAN / safe_name\n",
    "    clean_path.write_text(txt_clean, encoding=\"utf-8\")\n",
    "    print(f\"{raw_path.name} -> {safe_name}\")\n",
    "\n",
    "print(f\"\\nCleaned files in: {CLEAN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c254fdb",
   "metadata": {},
   "source": [
    "# Cellule 6 — Chunking (char-based \"safe\" + chevauchement)\n",
    "\n",
    "Sans dépendances externes. Plus tard vous pourrez passer en token-based si vous voulez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b0199db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 300):\n",
    "    \"\"\"\n",
    "    Découpe par paragraphes/phrases avec overlap.\n",
    "    chunk_size : 2000 chars ~ 300-500 tokens\n",
    "    overlap : 300 chars ~ 50-75 tokens pour continuité contextuelle\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    n = len(text)\n",
    "    start = 0\n",
    "    \n",
    "    while start < n:\n",
    "        end = min(n, start + chunk_size)\n",
    "        \n",
    "        # Stratégie de découpe (par ordre de priorité)\n",
    "        window = text[start:end]\n",
    "        \n",
    "        # Priorité max: paragraphe (double saut de ligne)\n",
    "        para_cut = window.rfind(\"\\n\\n\")\n",
    "        if para_cut > int(0.5 * len(window)):\n",
    "            end = start + para_cut + 2  # Garder les \\n\\n\n",
    "        \n",
    "        # Sinon: fin de phrase (. ! ?)\n",
    "        elif any(marker in window for marker in [\". \", \"! \", \"? \"]):\n",
    "            sentence_cuts = [\n",
    "                window.rfind(\". \"),\n",
    "                window.rfind(\"! \"),\n",
    "                window.rfind(\"? \")\n",
    "            ]\n",
    "            best_cut = max(sentence_cuts)\n",
    "            if best_cut > int(0.4 * len(window)):\n",
    "                end = start + best_cut + 2  # Inclure ponctuation + espace\n",
    "        \n",
    "        # Fallback: simple saut de ligne\n",
    "        elif \"\\n\" in window:\n",
    "            line_cut = window.rfind(\"\\n\")\n",
    "            if line_cut > int(0.3 * len(window)):\n",
    "                end = start + line_cut + 1\n",
    "        \n",
    "        # Extraire et nettoyer le chunk\n",
    "        chunk = text[start:end].strip()\n",
    "        \n",
    "        if chunk:\n",
    "            chunks.append((start, end, chunk))\n",
    "        \n",
    "        # Avancer avec overlap (sauf si on est à la fin)\n",
    "        if end >= n:\n",
    "            break\n",
    "        start = max(end - overlap, start + 1)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cd8ba",
   "metadata": {},
   "source": [
    "# Cellule 7 — Génération chunks.jsonl avec métadonnées + citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c55dea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks: 1472\n",
      "0 SKIP\n",
      "Wrote: data\\chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "def make_chunk_id(doc_id: str, idx: int) -> str:\n",
    "    return f\"{doc_id}_{idx:04d}\"\n",
    "\n",
    "df_docs = pd.read_csv(DOCS_CSV)\n",
    "out = CHUNKS_JSONL.open(\"w\", encoding=\"utf-8\")\n",
    "\n",
    "total_chunks = 0\n",
    "skipped = 0\n",
    "\n",
    "for _, row in df_docs.iterrows():\n",
    "    raw_path = Path(row[\"local_path\"])\n",
    "    \n",
    "    # Utiliser le nom normalisé pour chercher le fichier clean\n",
    "    safe_name = safe_filename(raw_path.stem) + \".txt\"\n",
    "    clean_path = CLEAN / safe_name\n",
    "    \n",
    "    if not clean_path.exists():\n",
    "        print(f\"SKIP (no clean): {clean_path}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    text = read_text(clean_path)\n",
    "    spans = chunk_text(text, chunk_size=2000, overlap=200)\n",
    "\n",
    "    for i, (s, e, chunk) in enumerate(spans):\n",
    "        rec = {\n",
    "            \"chunk_id\": make_chunk_id(row[\"doc_id\"], i),\n",
    "            \"doc_id\": row[\"doc_id\"],\n",
    "            \"text\": chunk,\n",
    "            \"title\": row.get(\"title\", \"\"),\n",
    "            \"date\": row.get(\"date\", \"\"),\n",
    "            \"author\": row.get(\"author\", \"\"),\n",
    "            \"source\": row.get(\"source\", \"\"),\n",
    "            \"url\": row.get(\"url\", \"\"),\n",
    "            \"start_char\": int(s),\n",
    "            \"end_char\": int(e),\n",
    "        }\n",
    "        out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        total_chunks += 1\n",
    "\n",
    "out.close()\n",
    "print(f\"\\nTotal chunks: {total_chunks}\")\n",
    "if skipped > 0:\n",
    "    print(f\"Skipped: {skipped} documents\")\n",
    "else:\n",
    "    print(\"0 SKIP\")\n",
    "print(f\"Wrote: {CHUNKS_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64dbc9",
   "metadata": {},
   "source": [
    "# Cellule 8 — Sanity checks (stats + exemples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f0d1570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONTROLE QUALITE - DATASET RAG\n",
      "============================================================\n",
      "\n",
      "Documents sources    : 6\n",
      "Chunks generes       : 1472\n",
      "Moyenne chunks/doc   : 245.3\n",
      "\n",
      "------------------------------------------------------------\n",
      "DISTRIBUTION DES LONGUEURS (caracteres)\n",
      "------------------------------------------------------------\n",
      "Moyenne              : 677 chars\n",
      "Mediane              : 161 chars\n",
      "Min                  : 1 chars\n",
      "Max                  : 1999 chars\n",
      "Ecart-type           : 787 chars\n",
      "\n",
      "Estimation tokens    : 169 tokens (moyenne)\n",
      "\n",
      "------------------------------------------------------------\n",
      "REPARTITION PAR DOCUMENT\n",
      "------------------------------------------------------------\n",
      "doc006 : 542 chunks - Témoignage (Lebrun)\n",
      "doc005 : 284 chunks - Les services statistiques français pendant l’Occup\n",
      "doc004 : 212 chunks - Le racisme hitlérien, machine de guerre contre la \n",
      "doc001 : 209 chunks - Armistice à Bordeaux\n",
      "doc002 : 156 chunks - Convention d’armistice franco-allemande\n",
      "doc003 :  69 chunks - Déclaration interalliée du 17 décembre 1942\n",
      "\n",
      "============================================================\n",
      "EXEMPLES ALEATOIRES (verification manuelle)\n",
      "============================================================\n",
      "\n",
      "--- CHUNK 1 ---\n",
      "ID       : doc006_0223\n",
      "Document : Témoignage (Lebrun)\n",
      "Date     : N/A\n",
      "Source   : Wikisource\n",
      "Longueur : 1649 chars\n",
      "Position : [346797 - 348449]\n",
      "\n",
      "Extrait (200 premiers caracteres):\n",
      "lendemains de la guerre. J’en ai gardé des impressions personnelles qui se sont avivées encore au cours des dernières années. C’est à les exposer que je voudrais consacrer les pages qui suivent.\n",
      "\n",
      "⁂\n",
      "\n",
      "P...\n",
      "\n",
      "--- CHUNK 2 ---\n",
      "ID       : doc005_0070\n",
      "Document : Les services statistiques français pendant l’Occupation\n",
      "Date     : N/A\n",
      "Source   : Wikisource\n",
      "Longueur : 1786 chars\n",
      "Position : [108300 - 110088]\n",
      "\n",
      "Extrait (200 premiers caracteres):\n",
      "ce répertoire d’identification n’a pas été établi pour la France et l’Algérie en application de la législation antisémite de Vichy !\n",
      "\n",
      "Les instructions complémentaires du 15 avril 1941 au 21 mai 1942\n",
      "\n",
      "...\n",
      "\n",
      "--- CHUNK 3 ---\n",
      "ID       : doc006_0185\n",
      "Document : Témoignage (Lebrun)\n",
      "Date     : N/A\n",
      "Source   : Wikisource\n",
      "Longueur : 1848 chars\n",
      "Position : [287886 - 289736]\n",
      "\n",
      "Extrait (200 premiers caracteres):\n",
      "dat et un gendarme, gardèrent en permanence les routes Vizille-Grenoble par Pont-de-Claix, Vizille-Grenoble par la route Napoléon, Vizille-Grenoble par Uriage, Vizille-Bourg-d’Oisans et Vizille-Gap.\n",
      "\n",
      "...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Charger les chunks\n",
    "rows = []\n",
    "with CHUNKS_JSONL.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "df_chunks = pd.DataFrame(rows)\n",
    "df_chunks[\"len_chars\"] = df_chunks[\"text\"].str.len()\n",
    "\n",
    "# STATISTIQUES GLOBALES\n",
    "print(\"=\" * 60)\n",
    "print(\"CONTROLE QUALITE - DATASET RAG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Nombre de documents et chunks\n",
    "nb_docs = df_chunks[\"doc_id\"].nunique()\n",
    "nb_chunks = len(df_chunks)\n",
    "print(f\"\\nDocuments sources    : {nb_docs}\")\n",
    "print(f\"Chunks generes       : {nb_chunks}\")\n",
    "print(f\"Moyenne chunks/doc   : {nb_chunks / nb_docs:.1f}\")\n",
    "\n",
    "# STATISTIQUES DE LONGUEUR\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"DISTRIBUTION DES LONGUEURS (caracteres)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Moyenne              : {df_chunks['len_chars'].mean():.0f} chars\")\n",
    "print(f\"Mediane              : {df_chunks['len_chars'].median():.0f} chars\")\n",
    "print(f\"Min                  : {df_chunks['len_chars'].min():.0f} chars\")\n",
    "print(f\"Max                  : {df_chunks['len_chars'].max():.0f} chars\")\n",
    "print(f\"Ecart-type           : {df_chunks['len_chars'].std():.0f} chars\")\n",
    "\n",
    "# Estimation en tokens (approximation : 1 token ~ 4 chars)\n",
    "print(f\"\\nEstimation tokens    : {df_chunks['len_chars'].mean() / 4:.0f} tokens (moyenne)\")\n",
    "\n",
    "# REPARTITION PAR DOCUMENT\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"REPARTITION PAR DOCUMENT\")\n",
    "print(\"-\" * 60)\n",
    "chunks_per_doc = df_chunks.groupby(\"doc_id\").size().sort_values(ascending=False)\n",
    "for doc_id, count in chunks_per_doc.items():\n",
    "    title = df_chunks[df_chunks[\"doc_id\"] == doc_id][\"title\"].iloc[0]\n",
    "    # Gérer les titres vides ou NaN\n",
    "    title_str = str(title) if pd.notna(title) and title else doc_id\n",
    "    print(f\"{doc_id} : {count:3d} chunks - {title_str[:50]}\")\n",
    "\n",
    "# EXEMPLES ALEATOIRES\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXEMPLES ALEATOIRES (verification manuelle)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_chunks = df_chunks.sample(min(3, len(df_chunks)))\n",
    "for idx, (_, chunk) in enumerate(sample_chunks.iterrows(), 1):\n",
    "    print(f\"\\n--- CHUNK {idx} ---\")\n",
    "    print(f\"ID       : {chunk['chunk_id']}\")\n",
    "    print(f\"Document : {chunk['title'] if pd.notna(chunk['title']) and chunk['title'] else chunk['doc_id']}\")\n",
    "    print(f\"Date     : {chunk['date'] if pd.notna(chunk['date']) and chunk['date'] else 'N/A'}\")\n",
    "    print(f\"Source   : {chunk['source']}\")\n",
    "    print(f\"Longueur : {chunk['len_chars']} chars\")\n",
    "    print(f\"Position : [{chunk['start_char']} - {chunk['end_char']}]\")\n",
    "    print(f\"\\nExtrait (200 premiers caracteres):\")\n",
    "    print(f\"{chunk['text'][:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e8e2b",
   "metadata": {},
   "source": [
    "# Cellule 9 — Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "77921de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks sans source ou url: 1472 / 1472\n",
      "Dataset prêt pour indexation.\n"
     ]
    }
   ],
   "source": [
    "assert DOCS_CSV.exists()\n",
    "assert CHUNKS_JSONL.exists()\n",
    "\n",
    "# Vérifier qu'on a bien des URLs/sources pour citer\n",
    "missing_sources = (df_chunks[\"source\"].fillna(\"\") == \"\") | (df_chunks[\"url\"].fillna(\"\") == \"\")\n",
    "print(\"Chunks sans source ou url:\", missing_sources.sum(), \"/\", len(df_chunks))\n",
    "\n",
    "# Vérifier que les chunks ne sont pas vides\n",
    "assert (df_chunks[\"text\"].str.strip().str.len() > 0).all()\n",
    "\n",
    "print(\"Dataset prêt pour indexation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
